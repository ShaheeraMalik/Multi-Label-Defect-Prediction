{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee481e8-e0e8-42ed-b933-e5ef21756c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Part 2: Multi-Label Defect Prediction (Python 3.13.3 Compatible)\n",
    "\n",
    "# %% \n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "import joblib\n",
    "\n",
    "# %% \n",
    "# 2. Load & inspect\n",
    "df = pd.read_csv(r'D:/6th Semester/Data Science/Assignmentno4/dataset.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Missing values per column:\\n\", df.isna().sum())\n",
    "\n",
    "# identify features & labels\n",
    "X_text     = df['report'].fillna('')\n",
    "label_cols = [c for c in df.columns if c.startswith('type_')]\n",
    "y_all      = df[label_cols].values\n",
    "print(\"Label distribution (all):\\n\", df[label_cols].sum())\n",
    "\n",
    "# %%  \n",
    "# 3. Train/val/test split (70/15/15)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_text, y_all, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.1765, random_state=42\n",
    ")  # â‰ˆ15% of overall data\n",
    "print(\"Splits shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape,   \"y_val:  \", y_val.shape)\n",
    "print(\"  X_test: \", X_test.shape,  \"y_test: \", y_test.shape)\n",
    "\n",
    "# %%  \n",
    "# 4. Drop labels that are constant in the training set\n",
    "train_pos_counts = y_train.sum(axis=0)\n",
    "mask_labels = (train_pos_counts > 0) & (train_pos_counts < y_train.shape[0])\n",
    "label_cols = [c for c, keep in zip(label_cols, mask_labels) if keep]\n",
    "y_train = y_train[:, mask_labels]\n",
    "y_val   = y_val[:,   mask_labels]\n",
    "y_test  = y_test[:,  mask_labels]\n",
    "print(\"Kept labels:\", label_cols)\n",
    "print(\"New y_train shape:\", y_train.shape)\n",
    "\n",
    "# %%  \n",
    "# 5. Precision@k helper\n",
    "def precision_at_k(y_true, y_scores, k=3):\n",
    "    n, _ = y_true.shape\n",
    "    accs = []\n",
    "    for i in range(n):\n",
    "        topk = np.argsort(y_scores[i])[-k:]\n",
    "        accs.append(y_true[i, topk].sum() / k)\n",
    "    return np.mean(accs)\n",
    "\n",
    "# %%  \n",
    "# 6. Logistic Regression (One-vs-Rest + GridSearchCV)\n",
    "tfidf_log = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "pipe_log  = Pipeline([\n",
    "    ('tfidf', tfidf_log),\n",
    "    ('clf',  OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "])\n",
    "param_log = {'clf__estimator__C': [0.01, 0.1, 1, 10]}\n",
    "grid_log  = GridSearchCV(pipe_log, param_log, cv=3, scoring='f1_micro')\n",
    "grid_log.fit(X_train, y_train)\n",
    "print(\"Best C (LogisticRegression):\", grid_log.best_params_)\n",
    "\n",
    "# %%  \n",
    "# 7. SVM (One-vs-Rest + GridSearchCV)\n",
    "tfidf_svm = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "pipe_svm  = Pipeline([\n",
    "    ('tfidf', tfidf_svm),\n",
    "    ('clf',  OneVsRestClassifier(SVC(kernel='linear', probability=True)))\n",
    "])\n",
    "param_svm = {'clf__estimator__C': [0.1, 1, 10]}\n",
    "grid_svm  = GridSearchCV(pipe_svm, param_svm, cv=3, scoring='f1_micro')\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(\"Best C (SVM):\", grid_svm.best_params_)\n",
    "\n",
    "# %%  \n",
    "# 8. Perceptron (batch) & online\n",
    "# Batch Perceptron\n",
    "tfidf_per = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "pipe_per  = Pipeline([\n",
    "    ('tfidf', tfidf_per),\n",
    "    ('clf',  OneVsRestClassifier(Perceptron(max_iter=1000)))\n",
    "])\n",
    "pipe_per.fit(X_train, y_train)\n",
    "\n",
    "# Online Perceptrons: one Perceptron per label\n",
    "vectorizer_online = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_mat = vectorizer_online.fit_transform(X_train).toarray()\n",
    "X_test_mat  = vectorizer_online.transform(X_test).toarray()\n",
    "\n",
    "n_labels     = y_train.shape[1]\n",
    "perceptrons  = [Perceptron(max_iter=1, warm_start=True) for _ in range(n_labels)]\n",
    "# Initialize each with a first partial_fit\n",
    "for idx, pp in enumerate(perceptrons):\n",
    "    pp.partial_fit(X_train_mat, y_train[:, idx], classes=[0,1])\n",
    "# Continue online updates for 5 epochs\n",
    "for _ in range(5):\n",
    "    for xi, yi in zip(X_train_mat, y_train):\n",
    "        xi = xi.reshape(1, -1)\n",
    "        for idx, pp in enumerate(perceptrons):\n",
    "            pp.partial_fit(xi, [yi[idx]])\n",
    "\n",
    "# %%  \n",
    "# 9. DNN via sklearnâ€™s MLPClassifier (2 hidden layers)\n",
    "tfidf_mlp = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "pipe_mlp  = Pipeline([\n",
    "    ('tfidf', tfidf_mlp),\n",
    "    ('clf',  OneVsRestClassifier(\n",
    "        MLPClassifier(\n",
    "            hidden_layer_sizes=(128,64),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            random_state=42,\n",
    "            max_iter=100,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "pipe_mlp.fit(X_train, y_train)\n",
    "\n",
    "# %%  \n",
    "# 10. Evaluation on test set\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_log   = grid_log.predict(X_test)\n",
    "y_score_log  = grid_log.decision_function(X_test)\n",
    "results['LogisticRegression'] = (y_pred_log, y_score_log)\n",
    "\n",
    "# SVM\n",
    "y_pred_svm   = grid_svm.predict(X_test)\n",
    "y_score_svm  = grid_svm.decision_function(X_test)\n",
    "results['SVM'] = (y_pred_svm, y_score_svm)\n",
    "\n",
    "# Perceptron (batch)\n",
    "y_pred_per   = pipe_per.predict(X_test)\n",
    "y_score_per  = pipe_per.decision_function(X_test)\n",
    "results['Perceptron'] = (y_pred_per, y_score_per)\n",
    "\n",
    "# Perceptron (online)\n",
    "y_pred_per_on  = np.column_stack([pp.predict(X_test_mat)    for pp in perceptrons])\n",
    "y_score_per_on = np.column_stack([pp.decision_function(X_test_mat) for pp in perceptrons])\n",
    "results['Perceptron(online)'] = (y_pred_per_on, y_score_per_on)\n",
    "\n",
    "# DNN (MLPClassifier)\n",
    "y_pred_mlp   = pipe_mlp.predict(X_test)\n",
    "y_score_mlp  = pipe_mlp.predict_proba(X_test)\n",
    "results['DNN (MLPClassifier)'] = (y_pred_mlp, y_score_mlp)\n",
    "\n",
    "# Print metrics with zero_division handling\n",
    "print(\"\\n=== Test Results ===\")\n",
    "for name, (y_pred, y_score) in results.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Hamming Loss :\", hamming_loss(y_test,                y_pred))\n",
    "    print(\"Micro F1     :\", f1_score(y_test,      y_pred, average='micro', zero_division=0))\n",
    "    print(\"Macro F1     :\", f1_score(y_test,      y_pred, average='macro', zero_division=0))\n",
    "    print(\"Precision@3  :\", precision_at_k(y_test, y_score, k=3))\n",
    "\n",
    "\n",
    "# after training:\n",
    "joblib.dump(grid_log.best_estimator_, \"pipe_log.pkl\")  # Logistic\n",
    "joblib.dump(grid_svm.best_estimator_, \"pipe_svm.pkl\")  # SVM\n",
    "joblib.dump(pipe_mlp,               \"pipe_mlp.pkl\")   # DNN/MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "# assume `results` dict is already defined as:\n",
    "#   results = {\n",
    "#       'LogisticRegression': (y_pred_log,   y_score_log),\n",
    "#       'SVM':                (y_pred_svm,   y_score_svm),\n",
    "#       'Perceptron':         (y_pred_per,   y_score_per),\n",
    "#       'Perceptron(online)': (y_pred_per_on,y_score_per_on),\n",
    "#       'DNN (MLP)':          (y_pred_mlp,   y_score_mlp),\n",
    "#   }\n",
    "\n",
    "rows = []\n",
    "for name, (y_pred, y_score) in results.items():\n",
    "    ham = hamming_loss(y_test, y_pred)\n",
    "    mic = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "    mac = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    p3  = precision_at_k(y_test, y_score, k=3)\n",
    "    rows.append({\n",
    "        'Model': name,\n",
    "        'Hamming Loss': ham,\n",
    "        'Micro-F1':     mic,\n",
    "        'Macro-F1':     mac,\n",
    "        'Precision@3':  p3\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(rows).set_index('Model')\n",
    "display(df_summary.sort_values('Micro-F1', ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
